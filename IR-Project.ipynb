{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjzU9vmFsL8Y",
    "outputId": "c35da6e5-ebfd-42e2-8ebb-79381ee1169e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\96279\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\96279\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSX2YIMDs4sH",
    "outputId": "994005eb-8d0d-448e-d8b9-8371c0fd4fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\96279\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: regex in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jHwxUx8tB_Q",
    "outputId": "57464022-049b-4fcf-ee60-0d43f08a2430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fpdf in c:\\users\\96279\\anaconda3\\lib\\site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSexKwJusCy9",
    "outputId": "5906cb22-664f-4ab2-d878-87a9e88af4d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\96279\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\96279\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\96279\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from fpdf import FPDF\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_articles_for_topic(topic, num_articles=10):\n",
    "    search_results = wikipedia.search(topic, results=num_articles)\n",
    "    articles = []\n",
    "    for title in search_results:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            articles.append({\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"content\": page.content\n",
    "            })\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            pass\n",
    "        except wikipedia.exceptions.PageError as e:\n",
    "            pass\n",
    "    return articles\n",
    "\n",
    "def generate_combined_summary_pdf(topics, num_articles_per_topic=10):\n",
    "    pdf = FPDF()\n",
    "\n",
    "    for topic in topics:\n",
    "        article_data = get_articles_for_topic(topic, num_articles=num_articles_per_topic)\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        pdf.cell(200, 10, txt=f\"Summary of {topic} Articles\", ln=True, align=\"C\")\n",
    "        pdf.cell(200, 10, txt=f\"Total Articles: {len(article_data)}\", ln=True)\n",
    "\n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "        for idx, article in enumerate(article_data, start=1):\n",
    "            words = article['content'].split()\n",
    "            unique_words = set(words)\n",
    "            num_stopwords = sum(1 for word in words if word.lower() in stopwords_set)\n",
    "\n",
    "            pdf.cell(200, 10, txt=f\"\\nArticle {idx} - {article['title']}\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"URL: {article['url']}\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Content:\\n{article['content'][:200]}\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Total Words: {len(words)}\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Unique Words: {len(unique_words)}\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Percentage of Unique Words: {(len(unique_words) / len(words)) * 100:.2f}%\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Percentage of Stopwords: {(num_stopwords / len(words)) * 100:.2f}%\", ln=True)\n",
    "\n",
    "    pdf.output(\"combined_articles_summary.pdf\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topics = [\"Artificial Intelligence\", \"Machine Learning\", \"Data Science\", \"Database\", \"Data mining\"]\n",
    "    generate_combined_summary_pdf(topics, num_articles_per_topic=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_VZKBIVtxm-",
    "outputId": "32eb5cc2-9083-435c-dff4-3f95d6682d04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\96279\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from fpdf import FPDF\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_articles_for_topic(topic, num_articles=10):\n",
    "    search_results = wikipedia.search(topic, results=num_articles)\n",
    "    articles = []\n",
    "    for title in search_results:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            articles.append({\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"content\": page.content\n",
    "            })\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            pass\n",
    "        except wikipedia.exceptions.PageError as e:\n",
    "            pass\n",
    "    return articles\n",
    "\n",
    "def generate_combined_summary_pdf(topics, num_articles_per_topic=10):\n",
    "    pdf = FPDF()\n",
    "\n",
    "    for topic in topics:\n",
    "        article_data = get_articles_for_topic(topic, num_articles=num_articles_per_topic)\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        pdf.cell(200, 10, txt=f\"Summary of {topic} Articles\", ln=True, align=\"C\")\n",
    "        pdf.cell(200, 10, txt=f\"Total Articles: {len(article_data)}\", ln=True)\n",
    "\n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "        for idx, article in enumerate(article_data, start=1):\n",
    "            words = article['content'].split()\n",
    "            unique_words = set(words)\n",
    "            num_stopwords = sum(1 for word in words if word.lower() in stopwords_set)\n",
    "\n",
    "            pdf.cell(200, 10, txt=f\"\\nArticle {idx} - {article['title']}\".encode('latin-1', 'replace').decode('latin-1'), ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"URL: {article['url']}\".encode('latin-1', 'replace').decode('latin-1'), ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Content:\\n{article['content'][:200]}\".encode('latin-1', 'replace').decode('latin-1'), ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Total Words: {len(words)}\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Unique Words: {len(unique_words)}\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Percentage of Unique Words: {(len(unique_words) / len(words)) * 100:.2f}%\", ln=True)\n",
    "            pdf.cell(200, 10, txt=f\"Percentage of Stopwords: {(num_stopwords / len(words)) * 100:.2f}%\", ln=True)\n",
    "\n",
    "    pdf.output(\"Articles-Summary.pdf\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topics = [\"Artificial Intelligence\", \"Machine Learning\", \"Data Science\", \"Database\", \"Data mining\"]\n",
    "    generate_combined_summary_pdf(topics, num_articles_per_topic=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr4LhIhtyJNT",
    "outputId": "d5ffa969-f283-49d9-c147-d99226536d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in c:\\users\\96279\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\96279\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\96279\\anaconda3\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\96279\\anaconda3\\lib\\site-packages (from wikipedia-api) (2.24.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\96279\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from scikit-learn) (1.19.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\96279\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "ZOcrkF6pyOWk",
    "outputId": "e76842f3-c8d1-4d67-f5a6-c897628978fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\96279\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\96279\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_articles_for_topic(topic, num_articles=10):\n",
    "    search_results = wikipedia.search(topic, results=num_articles)\n",
    "    articles = []\n",
    "    for title in search_results:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            articles.append({\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"content\": page.content\n",
    "            })\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            pass\n",
    "        except wikipedia.exceptions.PageError as e:\n",
    "            pass\n",
    "    return articles\n",
    "\n",
    "topics = [\"Artificial Intelligence\", \"Machine Learning\", \"Data Science\"]\n",
    "articles = []\n",
    "for topic in topics:\n",
    "    articles.extend(get_articles_for_topic(topic, num_articles=10))\n",
    "\n",
    "articles_df = pd.DataFrame(articles)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_df['content'])\n",
    "\n",
    "def search_articles(query, tfidf_matrix=tfidf_matrix, top_n=10):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "    top_article_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    return articles_df.iloc[top_article_indices]\n",
    "\n",
    "def on_search_button_click():\n",
    "    query = simpledialog.askstring(\"Input\", \"Enter your search query:\")\n",
    "    if query:\n",
    "        results = search_articles(query)\n",
    "        results_text.delete('1.0', tk.END)\n",
    "        for index, row in results.iterrows():\n",
    "            results_text.insert(tk.END, f\"{row['title']} - {row['url']}\\n\")\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Information Retrieval System\")\n",
    "\n",
    "search_button = tk.Button(root, text=\"Search\", command=on_search_button_click)\n",
    "search_button.pack()\n",
    "\n",
    "results_text = tk.Text(root, height=20, width=80)\n",
    "results_text.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
